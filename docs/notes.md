# Lab 1: Baseline Models - Strategy & Observations

## 1. Data Preparation Strategy

**Goal:** Train models on "Raw Data" to establish a baseline.

- **Cleaning Rules:**
  - Dropped all rows with missing values (`dropna`).
  - Dropped all non-numeric columns (e.g., "Day of Week") _except_ the Target variable.
- **Justification:**
  - We did **not** scale or normalize data (as per "Raw Data" definition).
  - We did **not** use One-Hot Encoding (to keep the feature space raw and simple).
  - This allows us to measure the true impact of feature engineering in future labs.

## 2. Dataset Specifics

- **Traffic (`TrafficTwoMonth.csv`):**
  - _Type:_ Multi-class Classification (Normal, Heavy, High, Low).
  - _Features:_ Purely numeric (CarCount, BikeCount). Ideal for baseline.
- **Accidents (`traffic_accidents.csv`):**
  - _Type:_ Binary/Multi-class (Injury vs. No Injury).
  - _Challenge:_ Heavily dependent on text features (Weather, Road Type) which were dropped. Expect lower baseline performance.
- **Economic (`economic_indicators...csv`):**
  - _Type:_ Classification (Predicting Country).
  - _Challenge:_ Contains negative numbers (GDP Growth), causing issues for Multinomial Naïve Bayes.
- **Flights (`flights.csv`):**
  - _Type:_ Binary (Delayed > 15min).
  - _Challenge:_ Massive dataset (millions of rows) + Unscaled numeric data.

## 3. Model Analysis & Observations

### A. Naïve Bayes

- **Best Variant:** `GaussianNB`.
- **Why:** It assumes a normal distribution of numeric data, which fits our continuous features (like Car Counts).
- **Issues:** `MultinomialNB` failed on Economic/Flights data because it cannot handle negative values (negative GDP/Delays).

### B. K-Nearest Neighbors (KNN)

- **Performance:** Good for small datasets (Traffic).
- **Failure:** Timed out on **Flights**.
- **Reason:** KNN is a "Lazy Learner." It calculates distances to _all_ training points at prediction time. With unscaled, massive flight data, this calculation is computationally impossible for a baseline script.

### C. Decision Trees

- **Performance:** Generally the most robust baseline model.
- **Why:** Trees do not care about "scaling." They just find split points (e.g., `CarCount > 50`). They work well even on raw, unnormalized data.

1. Understanding the Charts (Gini vs. Entropy)

Yes, the charts generated by that code are exactly what the lab means by "Hyperparameters study."

In a Decision Tree, the "Hyperparameters" are the settings you choose before training. The two most important ones are:

Max Depth (d): How many "questions" the tree can ask.

Low Depth (e.g., 2): Simple model ("Is it rush hour?").

High Depth (e.g., 25): Complex model ("Is it rush hour AND raining AND a Tuesday AND...").

Criterion (Gini vs. Entropy): The math formula the tree uses to decide which question to ask.

Entropy: A measure of "chaos." The tree tries to reduce chaos significantly with each split.

Gini: A measure of "purity." The tree tries to make the groups as pure as possible (e.g., all "Heavy Traffic" in one group).

How to read your chart:

X-Axis (Bottom): The complexity of the tree (Depth). As you go right, the tree gets larger.

Y-Axis (Left): The Accuracy. Higher is better.

The Lines:

If the Blue Line (Entropy) is higher than the Orange Line (Gini) at d=10, it means "Entropy" is the better setting for this specific dataset.

The Peak: Look for the highest point on the chart. If the highest point is at Depth=10 and Criterion=Entropy, those are the "Best Hyperparameters" you need to report.

### D. Logistic Regression

- **Performance:** Struggled on Accidents/Economic.
- **Convergence Warnings:** Frequent.
- **Reason:** Logistic Regression uses Gradient Descent. Without scaling (e.g., min-max scaling), the gradients explode or vanish, making it hard for the model to find the mathematical "bottom of the valley."
- **Flights:** Timed out due to the sheer complexity of optimizing unscaled weights on large data.

### E. Multi-Layer Perceptron (MLP)

- **Performance:** Poor (Flatline) for Traffic/Accidents.
- **Reason:** We used `SGD` (Stochastic Gradient Descent). Neural Networks **require** scaled inputs (0 to 1) to work effectively. On raw data, the gradients likely vanished, causing the network to stop learning immediately.
